{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4092f63-aefe-45b3-9869-13cabeda65f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## START HERE FOR CPGA FULL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "010e2d70-8e79-4f2b-a915-ccbd870aa58a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from: ./data/demo_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "######################## STEP 1: DATA PREPARATION ####\n",
    "### STEP 1 ###########################################\n",
    "######################################################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "DEMO_MODE = True\n",
    "\n",
    "if DEMO_MODE:\n",
    "    excel_path = './data/demo_dataset.xlsx'\n",
    "    image_dir = './data/images/LAYERS_EXTRACTED_CPGA'\n",
    "else:\n",
    "    excel_path = '/path/to/your/full_dataset.xlsx' \n",
    "    image_dir = '/path/to/your/full_images'\n",
    "\n",
    "if os.path.exists(excel_path):\n",
    "    data = pd.read_excel(excel_path)\n",
    "    print(f\"Loaded dataset from: {excel_path}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Dataset not found at {excel_path}. Please check README.\")\n",
    "\n",
    "geometry_mapping = {\n",
    "    'Primitive': 'R1',\n",
    "    'Diamond': 'R2',\n",
    "    'Gyroid': 'R3',\n",
    "    'Nevious': 'R4',\n",
    "    'FRD': 'R5',\n",
    "    'Fkoch': 'R6'\n",
    "}\n",
    "data['geometry_mapped'] = data['geometry'].map(geometry_mapping)\n",
    "\n",
    "def map_lattice_c(value):\n",
    "    if value in [0.3, 1.3, 0.8]:\n",
    "        return 1\n",
    "    elif value in [0.4, 1.4, 0.9]:\n",
    "        return 2\n",
    "    elif value in [0.5, 1.5, 1.0]:\n",
    "        return 3\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "data['lattice_c_mapped'] = data['lattice_c'].apply(map_lattice_c)\n",
    "\n",
    "def get_image_paths(lattice_n, lattice_c_mapped, geometry_mapped):\n",
    "    image_paths = []\n",
    "    for i in range(20):\n",
    "        image_name = f\"U{lattice_n}C{lattice_c_mapped}{geometry_mapped}_{i:02d}.png\"\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "        if os.path.exists(image_path):\n",
    "            image_paths.append(image_path)\n",
    "    return image_paths\n",
    "\n",
    "data['image_paths'] = data.apply(\n",
    "    lambda row: get_image_paths(row['lattice_n'], row['lattice_c_mapped'], row['geometry_mapped']),\n",
    "    axis=1\n",
    ")\n",
    "data = data[data['image_paths'].apply(lambda x: len(x) == 20)]\n",
    "\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  \n",
    "])\n",
    "\n",
    "def load_images(image_paths):\n",
    "    \"\"\"Load a stack of 20 grayscale images for one sample.\"\"\"\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        image = Image.open(image_path).convert('L')\n",
    "        transformed_image = image_transforms(image)\n",
    "        images.append(transformed_image)\n",
    "    images = torch.stack(images)          \n",
    "    images = images.permute(1, 0, 2, 3)      \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c1a13c-2da3-4f97-8add-8c87cf272182",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "######################## STEP 2: DATASET and MODEL ###\n",
    "### STEP 2 ###########################################\n",
    "######################################################\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, numeric_data, image_data, targets):\n",
    "        self.numeric_data = torch.tensor(numeric_data.values, dtype=torch.float32)\n",
    "        self.image_data = image_data\n",
    "        self.targets = torch.tensor(targets.values, dtype=torch.float32)\n",
    "\n",
    "        DEMO_MODE = True\n",
    "        \n",
    "        if DEMO_MODE:\n",
    "            self.original_image_dir = './data/images/LAYERS_EXTRACTED_CPGA'\n",
    "            self.convolved_image_dir = './data/images/LAYERS_EXTRACTED_CONV_CPGA'\n",
    "        else:\n",
    "            self.original_image_dir = '/path/to/your/original_images.xlsx' \n",
    "            self.convolved_image_dir = '/path/to/your/convolved_images'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        numeric_features = self.numeric_data[idx]\n",
    "        image_paths = self.image_data.iloc[idx]\n",
    "        \n",
    "        original_image_paths = [os.path.join(self.original_image_dir, os.path.basename(p)) for p in image_paths]\n",
    "        convolved_image_paths = [os.path.join(self.convolved_image_dir, os.path.basename(p)) for p in image_paths]\n",
    "\n",
    "        original_images = load_images(original_image_paths)\n",
    "        convolved_images = load_images(convolved_image_paths)\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        return numeric_features, original_images, convolved_images, target\n",
    "\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, image_size=150):\n",
    "        super().__init__()\n",
    "\n",
    "        self.final_spatial = image_size // (2 ** 4) \n",
    "\n",
    "        self.numeric_model = nn.Sequential(\n",
    "            nn.Linear(6, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        num_numeric_features = 32\n",
    "\n",
    "        self.film1_gamma = nn.Linear(num_numeric_features, 32) \n",
    "        self.film1_beta  = nn.Linear(num_numeric_features, 32)\n",
    "        \n",
    "        self.film2_gamma = nn.Linear(num_numeric_features, 64) \n",
    "        self.film2_beta  = nn.Linear(num_numeric_features, 64)\n",
    "        \n",
    "        self.film3_gamma = nn.Linear(num_numeric_features, 128) \n",
    "        self.film3_beta  = nn.Linear(num_numeric_features, 128)\n",
    "        \n",
    "        self.film4_gamma = nn.Linear(num_numeric_features, 256) \n",
    "        self.film4_beta  = nn.Linear(num_numeric_features, 256)\n",
    "       \n",
    "        self.orig_enc1 = nn.Sequential(nn.Conv3d(1, 32, 3, 1, 1), nn.InstanceNorm3d(32), nn.ReLU())\n",
    "        self.orig_enc2 = nn.Sequential(nn.Conv3d(32, 64, 3, 1, 1), nn.InstanceNorm3d(64), nn.ReLU())\n",
    "        self.orig_enc3 = nn.Sequential(nn.Conv3d(64, 128, 3, 1, 1), nn.InstanceNorm3d(128), nn.ReLU())\n",
    "        self.orig_enc4 = nn.Sequential(nn.Conv3d(128, 256, 3, 1, 1), nn.InstanceNorm3d(256), nn.ReLU())\n",
    "\n",
    "        self.conv_enc1 = nn.Sequential(nn.Conv3d(1, 32, 3, 1, 1), nn.InstanceNorm3d(32), nn.ReLU())\n",
    "        self.conv_enc2 = nn.Sequential(nn.Conv3d(32, 64, 3, 1, 1), nn.InstanceNorm3d(64), nn.ReLU())\n",
    "        self.conv_enc3 = nn.Sequential(nn.Conv3d(64, 128, 3, 1, 1), nn.InstanceNorm3d(128), nn.ReLU())\n",
    "        self.conv_enc4 = nn.Sequential(nn.Conv3d(128, 256, 3, 1, 1), nn.InstanceNorm3d(256), nn.ReLU())\n",
    "        \n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "\n",
    "        num_image_channels = 256 + 256\n",
    "        self.fusion_head = nn.Sequential(\n",
    "            nn.Conv3d(num_image_channels, 256, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * self.final_spatial * self.final_spatial, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def apply_film(self, x, gamma, beta):\n",
    "\n",
    "        gamma = gamma.view(gamma.shape[0], -1, 1, 1, 1)\n",
    "        beta = beta.view(beta.shape[0], -1, 1, 1, 1)\n",
    "        return (gamma * x) + beta\n",
    "\n",
    "    def forward(self, numeric_data, original_image_data, convolved_image_data):\n",
    "\n",
    "        f_num = self.numeric_model(numeric_data) \n",
    "\n",
    "        g1, b1 = self.film1_gamma(f_num), self.film1_beta(f_num)\n",
    "        g2, b2 = self.film2_gamma(f_num), self.film2_beta(f_num)\n",
    "        g3, b3 = self.film3_gamma(f_num), self.film3_beta(f_num)\n",
    "        g4, b4 = self.film4_gamma(f_num), self.film4_beta(f_num)\n",
    "        \n",
    "        f_orig = original_image_data\n",
    "        f_conv = convolved_image_data\n",
    "\n",
    "        f_orig = self.orig_enc1(f_orig)\n",
    "        f_conv = self.conv_enc1(f_conv)\n",
    "        f_orig = self.apply_film(f_orig, g1, b1)\n",
    "        f_conv = self.apply_film(f_conv, g1, b1)\n",
    "        f_orig, f_conv = self.pool(f_orig), self.pool(f_conv)\n",
    "\n",
    "        f_orig = self.orig_enc2(f_orig)\n",
    "        f_conv = self.conv_enc2(f_conv)\n",
    "        f_orig = self.apply_film(f_orig, g2, b2)\n",
    "        f_conv = self.apply_film(f_conv, g2, b2)\n",
    "        f_orig, f_conv = self.pool(f_orig), self.pool(f_conv)\n",
    "\n",
    "        f_orig = self.orig_enc3(f_orig)\n",
    "        f_conv = self.conv_enc3(f_conv)\n",
    "        f_orig = self.apply_film(f_orig, g3, b3)\n",
    "        f_conv = self.apply_film(f_conv, g3, b3)\n",
    "        f_orig, f_conv = self.pool(f_orig), self.pool(f_conv)\n",
    "\n",
    "        f_orig = self.orig_enc4(f_orig)\n",
    "        f_conv = self.conv_enc4(f_conv)\n",
    "        f_orig = self.apply_film(f_orig, g4, b4)\n",
    "        f_conv = self.apply_film(f_conv, g4, b4)\n",
    "        f_orig, f_conv = self.pool(f_orig), self.pool(f_conv)\n",
    "\n",
    "        img_feats = torch.cat([f_orig, f_conv], dim=1)\n",
    "        output = self.fusion_head(img_feats)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f6012af-f101-494c-98f4-f679fd2c801a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing numerical features...\n",
      "Normalization complete.\n"
     ]
    }
   ],
   "source": [
    "######################## STEP 3: TRAINING UTILITIES ##\n",
    "### STEP 3 ###########################################\n",
    "######################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import joblib \n",
    "\n",
    "X_numeric = data[['sav_ratio', 'void_ratio', 'lattice_n', 'Layer height', 'Intensity', 'mass_before']]\n",
    "X_images = data['image_paths']\n",
    "y = data['degree_of_conversion']\n",
    "\n",
    "X_numeric = X_numeric.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "X_num_train, X_num_test, X_img_train, X_img_test, y_train, y_test = train_test_split(\n",
    "    X_numeric, X_images, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_num_train, X_num_val, X_img_train, X_img_val, y_train, y_val = train_test_split(\n",
    "    X_num_train, X_img_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Normalizing numerical features...\")\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_num_train)\n",
    "\n",
    "joblib.dump(scaler, 'numeric_scaler.pkl')\n",
    "\n",
    "numeric_cols = X_num_train.columns\n",
    "def scale_df(df):\n",
    "    return pd.DataFrame(scaler.transform(df), columns=numeric_cols, index=df.index)\n",
    "\n",
    "X_num_train, X_num_val, X_num_test = map(scale_df, [X_num_train, X_num_val, X_num_test])\n",
    "print(\"Normalization complete.\")\n",
    "\n",
    "y_train = pd.to_numeric(y_train, errors='coerce').fillna(0)\n",
    "y_val   = pd.to_numeric(y_val, errors='coerce').fillna(0)\n",
    "y_test  = pd.to_numeric(y_test, errors='coerce').fillna(0)\n",
    "\n",
    "y_train_log, y_val_log, y_test_log = map(np.log1p, [y_train, y_val, y_test])\n",
    "\n",
    "train_dataset_log = MultimodalDataset(X_num_train, X_img_train, y_train_log)\n",
    "val_dataset_log   = MultimodalDataset(X_num_val,   X_img_val,   y_val_log)\n",
    "test_dataset_log  = MultimodalDataset(X_num_test,  X_img_test,  y_test_log)\n",
    "\n",
    "train_loader_log = DataLoader(train_dataset_log, batch_size=32, shuffle=True,  num_workers=8, pin_memory=True)\n",
    "val_loader_log   = DataLoader(val_dataset_log,   batch_size=32, shuffle=False, num_workers=8, pin_memory=True)\n",
    "test_loader_log  = DataLoader(test_dataset_log,  batch_size=32, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultimodalModel(image_size=150).to(device)\n",
    "\n",
    "def weighted_mse_loss(output, target):\n",
    "    weight = torch.ones_like(target)\n",
    "    weight = torch.where((target >= 0.8) & (target < 0.87), 4, weight)\n",
    "    weight = torch.where((target >= 0.7) & (target < 0.8), 5, weight)\n",
    "    weight = torch.where((target >= 0.6) & (target < 0.7), 6, weight)\n",
    "    weight = torch.where(target <= 0.6, 7, weight)\n",
    "    return (weight * (output - target) ** 2).mean()\n",
    "\n",
    "criterion = weighted_mse_loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00015)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "best_val_loss = float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e602f896-651e-49c9-b98e-7bfda128a963",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cuda with mixed precision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_816/823269859.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/tmp/ipykernel_816/823269859.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/mnt/d/Ubuntu/miniconda3/envs/torch5090/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_816/823269859.py:50: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] | Time: 3.3s | Train Loss: 1.98814 | Val Loss: 1.85232 | Best: 1.85232\n",
      "Epoch [5/500] | Time: 1.2s | Train Loss: 0.35715 | Val Loss: 0.17510 | Best: 0.17510\n",
      "Epoch [10/500] | Time: 1.1s | Train Loss: 0.02509 | Val Loss: 0.08858 | Best: 0.01747\n",
      "Epoch [15/500] | Time: 1.1s | Train Loss: 0.20688 | Val Loss: 0.02483 | Best: 0.01747\n",
      "Epoch [20/500] | Time: 1.1s | Train Loss: 0.04803 | Val Loss: 0.01233 | Best: 0.01233\n",
      "Epoch [25/500] | Time: 1.1s | Train Loss: 0.08565 | Val Loss: 0.02441 | Best: 0.00904\n",
      "Epoch [30/500] | Time: 1.1s | Train Loss: 0.02682 | Val Loss: 0.00597 | Best: 0.00597\n",
      "Epoch [35/500] | Time: 1.1s | Train Loss: 0.06680 | Val Loss: 0.00585 | Best: 0.00493\n",
      "Epoch [40/500] | Time: 1.2s | Train Loss: 0.05501 | Val Loss: 0.02823 | Best: 0.00453\n",
      "Epoch [45/500] | Time: 1.1s | Train Loss: 0.03563 | Val Loss: 0.00758 | Best: 0.00453\n",
      "Epoch [50/500] | Time: 1.2s | Train Loss: 0.01097 | Val Loss: 0.01669 | Best: 0.00453\n",
      "Epoch [55/500] | Time: 1.1s | Train Loss: 0.08167 | Val Loss: 0.03499 | Best: 0.00453\n",
      "Epoch [60/500] | Time: 1.2s | Train Loss: 0.03904 | Val Loss: 0.03325 | Best: 0.00453\n",
      "Epoch [65/500] | Time: 1.1s | Train Loss: 0.02223 | Val Loss: 0.01443 | Best: 0.00453\n",
      "Epoch [70/500] | Time: 1.2s | Train Loss: 0.07449 | Val Loss: 0.01630 | Best: 0.00453\n",
      "Epoch [75/500] | Time: 1.1s | Train Loss: 0.04252 | Val Loss: 0.02417 | Best: 0.00453\n",
      "Epoch [80/500] | Time: 1.1s | Train Loss: 0.02157 | Val Loss: 0.01400 | Best: 0.00453\n",
      "Epoch [85/500] | Time: 1.1s | Train Loss: 0.05726 | Val Loss: 0.01445 | Best: 0.00453\n",
      "Epoch [90/500] | Time: 1.1s | Train Loss: 0.06238 | Val Loss: 0.01826 | Best: 0.00453\n",
      "Epoch [95/500] | Time: 1.2s | Train Loss: 0.07508 | Val Loss: 0.01826 | Best: 0.00453\n",
      "Epoch [100/500] | Time: 1.1s | Train Loss: 0.07674 | Val Loss: 0.02821 | Best: 0.00453\n",
      "Epoch [105/500] | Time: 1.1s | Train Loss: 0.02766 | Val Loss: 0.01688 | Best: 0.00453\n",
      "Epoch [110/500] | Time: 1.2s | Train Loss: 0.05674 | Val Loss: 0.01137 | Best: 0.00453\n",
      "Epoch [115/500] | Time: 1.2s | Train Loss: 0.02193 | Val Loss: 0.00556 | Best: 0.00453\n",
      "Epoch [120/500] | Time: 0.1s | Train Loss: 0.05458 | Val Loss: 0.01439 | Best: 0.00453\n",
      "Epoch [125/500] | Time: 1.1s | Train Loss: 0.06091 | Val Loss: 0.00934 | Best: 0.00453\n",
      "Epoch [130/500] | Time: 1.2s | Train Loss: 0.12246 | Val Loss: 0.00569 | Best: 0.00453\n",
      "Epoch [135/500] | Time: 1.1s | Train Loss: 0.06656 | Val Loss: 0.00795 | Best: 0.00453\n",
      "Epoch [140/500] | Time: 1.1s | Train Loss: 0.05481 | Val Loss: 0.00841 | Best: 0.00453\n",
      "Epoch [145/500] | Time: 1.1s | Train Loss: 0.05459 | Val Loss: 0.00919 | Best: 0.00453\n",
      "Epoch [150/500] | Time: 1.1s | Train Loss: 0.04871 | Val Loss: 0.00974 | Best: 0.00453\n",
      "Epoch [155/500] | Time: 1.2s | Train Loss: 0.05393 | Val Loss: 0.00844 | Best: 0.00453\n",
      "Epoch [160/500] | Time: 1.1s | Train Loss: 0.03018 | Val Loss: 0.00754 | Best: 0.00453\n",
      "Epoch [165/500] | Time: 1.1s | Train Loss: 0.09846 | Val Loss: 0.00673 | Best: 0.00453\n",
      "Epoch [170/500] | Time: 1.1s | Train Loss: 0.07152 | Val Loss: 0.00815 | Best: 0.00453\n",
      "Epoch [175/500] | Time: 1.1s | Train Loss: 0.03848 | Val Loss: 0.00943 | Best: 0.00453\n",
      "Epoch [180/500] | Time: 1.1s | Train Loss: 0.01294 | Val Loss: 0.01163 | Best: 0.00453\n",
      "Epoch [185/500] | Time: 1.2s | Train Loss: 0.06302 | Val Loss: 0.01374 | Best: 0.00453\n",
      "Epoch [190/500] | Time: 1.1s | Train Loss: 0.01715 | Val Loss: 0.01193 | Best: 0.00453\n",
      "Epoch [195/500] | Time: 1.2s | Train Loss: 0.01422 | Val Loss: 0.01099 | Best: 0.00453\n",
      "Epoch [200/500] | Time: 1.1s | Train Loss: 0.05924 | Val Loss: 0.01020 | Best: 0.00453\n",
      "Epoch [205/500] | Time: 0.1s | Train Loss: 0.04700 | Val Loss: 0.00927 | Best: 0.00453\n",
      "Epoch [210/500] | Time: 1.1s | Train Loss: 0.03292 | Val Loss: 0.00896 | Best: 0.00453\n",
      "Epoch [215/500] | Time: 1.1s | Train Loss: 0.04307 | Val Loss: 0.01244 | Best: 0.00453\n",
      "Epoch [220/500] | Time: 1.2s | Train Loss: 0.02052 | Val Loss: 0.01788 | Best: 0.00453\n",
      "Epoch [225/500] | Time: 1.1s | Train Loss: 0.01849 | Val Loss: 0.01314 | Best: 0.00453\n",
      "Epoch [230/500] | Time: 1.1s | Train Loss: 0.07385 | Val Loss: 0.00959 | Best: 0.00453\n",
      "Epoch [235/500] | Time: 1.2s | Train Loss: 0.03741 | Val Loss: 0.01011 | Best: 0.00453\n",
      "Epoch [240/500] | Time: 1.2s | Train Loss: 0.02694 | Val Loss: 0.01124 | Best: 0.00453\n",
      "Epoch [245/500] | Time: 1.1s | Train Loss: 0.02160 | Val Loss: 0.01343 | Best: 0.00453\n",
      "Epoch [250/500] | Time: 1.2s | Train Loss: 0.00717 | Val Loss: 0.01567 | Best: 0.00453\n",
      "Epoch [255/500] | Time: 1.1s | Train Loss: 0.03944 | Val Loss: 0.01632 | Best: 0.00453\n",
      "Epoch [260/500] | Time: 1.1s | Train Loss: 0.04284 | Val Loss: 0.01265 | Best: 0.00453\n",
      "Epoch [265/500] | Time: 1.2s | Train Loss: 0.03077 | Val Loss: 0.01038 | Best: 0.00453\n",
      "Epoch [270/500] | Time: 1.2s | Train Loss: 0.03275 | Val Loss: 0.01010 | Best: 0.00453\n",
      "Epoch [275/500] | Time: 1.1s | Train Loss: 0.05233 | Val Loss: 0.00987 | Best: 0.00453\n",
      "Epoch [280/500] | Time: 1.1s | Train Loss: 0.05648 | Val Loss: 0.00959 | Best: 0.00453\n",
      "Epoch [285/500] | Time: 1.1s | Train Loss: 0.02711 | Val Loss: 0.00997 | Best: 0.00453\n",
      "Epoch [290/500] | Time: 1.1s | Train Loss: 0.04178 | Val Loss: 0.01041 | Best: 0.00453\n",
      "Epoch [295/500] | Time: 1.2s | Train Loss: 0.05045 | Val Loss: 0.00958 | Best: 0.00453\n",
      "Epoch [300/500] | Time: 1.2s | Train Loss: 0.04700 | Val Loss: 0.00903 | Best: 0.00453\n",
      "Epoch [305/500] | Time: 1.1s | Train Loss: 0.04633 | Val Loss: 0.00841 | Best: 0.00453\n",
      "Epoch [310/500] | Time: 1.1s | Train Loss: 0.06814 | Val Loss: 0.00822 | Best: 0.00453\n",
      "Epoch [315/500] | Time: 1.1s | Train Loss: 0.04431 | Val Loss: 0.00836 | Best: 0.00453\n",
      "Epoch [320/500] | Time: 1.1s | Train Loss: 0.00734 | Val Loss: 0.00813 | Best: 0.00453\n",
      "Epoch [325/500] | Time: 1.2s | Train Loss: 0.01327 | Val Loss: 0.00750 | Best: 0.00453\n",
      "Epoch [330/500] | Time: 1.1s | Train Loss: 0.02008 | Val Loss: 0.00690 | Best: 0.00453\n",
      "Epoch [335/500] | Time: 1.1s | Train Loss: 0.03564 | Val Loss: 0.00657 | Best: 0.00453\n",
      "Epoch [340/500] | Time: 1.1s | Train Loss: 0.06393 | Val Loss: 0.00719 | Best: 0.00453\n",
      "Epoch [345/500] | Time: 1.1s | Train Loss: 0.03791 | Val Loss: 0.00798 | Best: 0.00453\n",
      "Epoch [350/500] | Time: 1.2s | Train Loss: 0.03847 | Val Loss: 0.00808 | Best: 0.00453\n",
      "Epoch [355/500] | Time: 1.1s | Train Loss: 0.00847 | Val Loss: 0.00876 | Best: 0.00453\n",
      "Epoch [360/500] | Time: 1.1s | Train Loss: 0.02667 | Val Loss: 0.00904 | Best: 0.00453\n",
      "Epoch [365/500] | Time: 1.2s | Train Loss: 0.02881 | Val Loss: 0.00893 | Best: 0.00453\n",
      "Epoch [370/500] | Time: 1.1s | Train Loss: 0.04694 | Val Loss: 0.00915 | Best: 0.00453\n",
      "Epoch [375/500] | Time: 1.2s | Train Loss: 0.02032 | Val Loss: 0.00882 | Best: 0.00453\n",
      "Epoch [380/500] | Time: 1.1s | Train Loss: 0.04106 | Val Loss: 0.00888 | Best: 0.00453\n",
      "Epoch [385/500] | Time: 1.1s | Train Loss: 0.01425 | Val Loss: 0.00850 | Best: 0.00453\n",
      "Epoch [390/500] | Time: 1.1s | Train Loss: 0.02847 | Val Loss: 0.00874 | Best: 0.00453\n",
      "Epoch [395/500] | Time: 1.1s | Train Loss: 0.06839 | Val Loss: 0.00861 | Best: 0.00453\n",
      "Epoch [400/500] | Time: 1.2s | Train Loss: 0.01590 | Val Loss: 0.00849 | Best: 0.00453\n",
      "Epoch [405/500] | Time: 0.1s | Train Loss: 0.03233 | Val Loss: 0.00863 | Best: 0.00453\n",
      "Epoch [410/500] | Time: 1.1s | Train Loss: 0.03475 | Val Loss: 0.00849 | Best: 0.00453\n",
      "Epoch [415/500] | Time: 1.1s | Train Loss: 0.02317 | Val Loss: 0.00840 | Best: 0.00453\n",
      "Epoch [420/500] | Time: 1.1s | Train Loss: 0.05578 | Val Loss: 0.00826 | Best: 0.00453\n",
      "Epoch [425/500] | Time: 1.0s | Train Loss: 0.04116 | Val Loss: 0.00803 | Best: 0.00453\n",
      "Epoch [430/500] | Time: 1.2s | Train Loss: 0.04629 | Val Loss: 0.00817 | Best: 0.00453\n",
      "Epoch [435/500] | Time: 1.2s | Train Loss: 0.05268 | Val Loss: 0.00795 | Best: 0.00453\n",
      "Epoch [440/500] | Time: 1.1s | Train Loss: 0.03604 | Val Loss: 0.00791 | Best: 0.00453\n",
      "Epoch [445/500] | Time: 1.1s | Train Loss: 0.00522 | Val Loss: 0.00798 | Best: 0.00453\n",
      "Epoch [450/500] | Time: 1.1s | Train Loss: 0.02779 | Val Loss: 0.00782 | Best: 0.00453\n",
      "Epoch [455/500] | Time: 1.1s | Train Loss: 0.04484 | Val Loss: 0.00782 | Best: 0.00453\n",
      "Epoch [460/500] | Time: 1.1s | Train Loss: 0.04123 | Val Loss: 0.00788 | Best: 0.00453\n",
      "Epoch [465/500] | Time: 1.2s | Train Loss: 0.09314 | Val Loss: 0.00808 | Best: 0.00453\n",
      "Epoch [470/500] | Time: 1.1s | Train Loss: 0.02768 | Val Loss: 0.00804 | Best: 0.00453\n",
      "Epoch [475/500] | Time: 1.1s | Train Loss: 0.02939 | Val Loss: 0.00813 | Best: 0.00453\n",
      "Epoch [480/500] | Time: 1.1s | Train Loss: 0.01527 | Val Loss: 0.00827 | Best: 0.00453\n",
      "Epoch [485/500] | Time: 1.1s | Train Loss: 0.01673 | Val Loss: 0.00860 | Best: 0.00453\n",
      "Epoch [490/500] | Time: 1.1s | Train Loss: 0.02541 | Val Loss: 0.00879 | Best: 0.00453\n",
      "Epoch [495/500] | Time: 1.1s | Train Loss: 0.01473 | Val Loss: 0.00909 | Best: 0.00453\n",
      "Epoch [500/500] | Time: 1.2s | Train Loss: 0.01842 | Val Loss: 0.00926 | Best: 0.00453\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "######################## STEP 4: RUN TRAINING ########\n",
    "### STEP 4 ###########################################\n",
    "######################################################\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "early_stopping_patience = 20  \n",
    "scaler = GradScaler()          \n",
    "\n",
    "log_file = open(\"training_log_CPGA.txt\", \"a\", buffering=1)\n",
    "\n",
    "print(f\"Starting training on {device} with mixed precision...\")\n",
    "\n",
    "for epoch in range(500):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    for batch_idx, (numeric_features, original_images, convolved_images, targets) in enumerate(train_loader_log):\n",
    "        numeric_features = numeric_features.to(device, non_blocking=True)\n",
    "        original_images = original_images.to(device, non_blocking=True)\n",
    "        convolved_images = convolved_images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(numeric_features, original_images, convolved_images)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader_log)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for numeric_features, original_images, convolved_images, targets in val_loader_log:\n",
    "            numeric_features = numeric_features.to(device, non_blocking=True)\n",
    "            original_images = original_images.to(device, non_blocking=True)\n",
    "            convolved_images = convolved_images.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(numeric_features, original_images, convolved_images)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader_log)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "        }, 'best_model_CPGA.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        msg = (f\"Epoch [{epoch+1}/500] | Time: {epoch_time:.1f}s | \"\n",
    "               f\"Train Loss: {avg_train_loss:.5f} | Val Loss: {avg_val_loss:.5f} | \"\n",
    "               f\"Best: {best_val_loss:.5f}\")\n",
    "        print(msg)\n",
    "        log_file.write(msg + \"\\n\")\n",
    "\n",
    "log_file.close()\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a2ce92-6df5-4102-a35c-82a5f297631d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch5090)",
   "language": "python",
   "name": "torch5090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
